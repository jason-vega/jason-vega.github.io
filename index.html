<!DOCTYPE html>
<head>
    <title>Jason Vega - CS PhD Student @ UIUC</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" type="image/png" href="favicon.png">
</head>
<body>
    <a href="../index.html" id="home">
        <h1>
            Jason Vega
        </h1>
    </a>
    <img src="headshot.jpeg" id="headshot" />
    <br>
    <br>
    <a href="CV.pdf">CV</a> &#183;
    <a href="https://medium.com/@jasonvega14">Medium</a> &#183;
    <a href="https://www.threads.net/@_jasonvega">Threads</a> &#183;
    <a href="https://www.linkedin.com/in/jason-vega/">LinkedIn</a> &#183;
    <a href="https://github.com/jason-vega">GitHub</a> &#183;
    <a href="mailto:javega3@illinois.edu">Email</a>
    <div id="about">
        <p>
            Hi there! I'm a second-year computer science Ph.D. student 
            at the <a href="https://cs.illinois.edu">University of Illinois 
            Urbana-Champaign</a> working on artificial intelligence research, 
            particularly on topics in trustworthy machine learning.
            I'm a member of the <a href="https://ggndpsngh.github.io">FOrmally
            Certified Automation and Learning (FOCAL) Lab</a>, where I'm advised
            by Prof. Gagandeep Singh. I graduated from the
            <a href="https://cse.ucsd.edu">University of California San Diego</a>
            in June 2022 with a B.S. in Computer Science. My research vision is to
            enable efficient, ethical development of intelligent systems that are 
            highly performant yet safe, transparent and ultimately beneficial 
            to humanity.
            <br>
            <br>
            Lately, I've been interested in <b>out-of-distribution generalization</b>; specifically, how computer vision models 
            can be more robust against real-world distribution shifts. To this end, I'm currently exploring how to improve average-case robustness against
            <a href="https://arxiv.org/pdf/1903.12261">common corruptions</a> 
            in image classification. Although some may argue that average-case robustness is <a href="https://proceedings.neurips.cc/paper/2021/file/ea4c796cccfc3899b5f9ae2874237c20-Paper.pdf">not as strong</a> as worst-case robustness, I believe
            that it is sufficient in practice for certain applications such as 
            autonomous vehicles, where the "adversary" is better modeled as a 
            probability distribution rather than an optimizer. Moreover, 
            although certain strategies have been devised to improve 
            robustness in this setting, I believe such solutions are not
            satisfactory at a fundamental level:
            <br>
            <div style="color: rgb(113, 113, 113); padding-left: 10px;">
                The predominant approach for improving common corruption 
                robustness has been to craft various data augmentation
                <a href="https://arxiv.org/pdf/1912.02781.pdf">schemes</a>. 
                However, under this strategy, a model's robustness towards a 
                corruption is largely
                <a href="https://arxiv.org/pdf/2102.11273.pdf">correlated</a> 
                with the similarity of augmentations present during training.
                Even the success of <a href="https://arxiv.org/pdf/2112.13547">state of the art</a> data augmentation schemes 
                can largely be explained by their ability to approximate 
                test-time corruptions. Because the goal of the common 
                corruption benchmark is to achieve robustness without 
                leveraging knowledge of the test-time corruptions during
                training, in some sense data augmentation does not provide 
                much meaningful progress towards this endeavor. Another 
                approach has been to perform self-supervised pre-training, 
                where it has been observed that robustness emerges and improves 
                with increasing
                <a href="https://arxiv.org/pdf/2111.06377.pdf">model</a> 
                <a href="https://arxiv.org/pdf/2304.07193v1.pdf">size</a>. 
                However, training large models is costly, and so such a 
                direction would limit the development of robust foundational
                models to only those with vast resources. How can we close the 
                gap in a more efficient manner than the current "robustness 
                from scaling" approach? Can robustness be truly achieved beyond 
                the training distribution?
            </div>
            <br>
            A bit more about me: I grew up in the Bay Area and will always be a 
            Californian at heart. Outside of research, I enjoy playing the violin in the <a href="https://music.illinois.edu/perform/orchestras/philharmonia-orchestra/">UIUC Philharmonia Orchestra</a>, going for a run, and watching films and shows.
        </p>
    </div>
</body>
</html>
