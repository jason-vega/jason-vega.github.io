<!DOCTYPE html>
    <title>Jason Vega - CS PhD @ UIUC</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h1>
        Jason Vega
    </h1>
    <img src="headshot.jpeg" id="headshot" />
    <div id="about">
        <p>
            Hi there! I'm a rising second-year computer science Ph.D. student 
            at the <a href="https://cs.illinois.edu">University of Illinois 
            Urbana-Champaign</a> working on artificial intelligence research, 
            particularly on topics in trustworthy machine learning.
            I'm a member of the <a href="https://ggndpsngh.github.io">FOrmally
            Certified Automation and Learning (FOCAL) Lab</a>, where I'm advised
            by Prof. Gagandeep Singh. I graduated from the
            <a href="https://cse.ucsd.edu">University of California San Diego</a>
            in June 2022 with a B.S. in Computer Science. My research vision is to
            enable efficient, ethical development of intelligent systems that are 
            highly performant yet safe, transparent and ultimiately beneficial 
            to humanity.
            <br>
            <br>
            Lately, I've been interested in how computer vision systems 
            can be more robust for real-world deployment. To this end, I'm currently 
            exploring how to improve
            <a href="https://arxiv.org/pdf/1903.12261">common corruption</a> 
            robustness in image classification:
            <br>
            <div style="color: rgb(113, 113, 113); padding-left: 10px;">
                The dominant paradigm for improving common corruption robustness
                has been to craft various data augmentation
                <a href="https://arxiv.org/pdf/1912.02781.pdf">schemes</a>, or to
                train with <a href="https://arxiv.org/pdf/2111.06377.pdf">larger</a> 
                models and/or
                <a href="https://arxiv.org/pdf/2304.07193v1.pdf">larger</a> amounts
                of data. Even so, a model's robustness towards a corruption is
                largely
                <a href="https://arxiv.org/pdf/2102.11273.pdf">correlated</a> with
                the similarity of augmentations present during training, and still
                cannot match the level of accuracy on clean data. How can we close 
                the gap in a more efficient manner than the current "robustness 
                from scaling" approach? Can robustness be truly achieved beyond
                the training distribution?
            </div>
            <br>
            A bit more about me: I grew up in the Bay Area and will always be a 
            Californian at heart. Outside of research, I enjoy playing the violin,
            going for a run, or watching films and shows.
            <br>
            <br>
            <a href="https://www.threads.net/@_jasonvega">Threads</a>
            <br>
            <a href="https://www.linkedin.com/in/jason-vega/">LinkedIn</a>
            <br>
            <a href="https://github.com/jason-vega">GitHub</a>
            <br>
            <a href="mailto:javega3@illinois.edu">Email</a>
        </p>
    </div>
</body>
</html>
